{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9e8e01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import csv\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cf7a1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_dir = Path(\"metadata\")\n",
    "json_files = list(input_dir.glob(\"*.json\"))\n",
    "\n",
    "all_keys = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acfa01bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1032524/1032524 [06:22<00:00, 2699.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovered 3004 columns.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def get_keys_from_file(path):\n",
    "    try:\n",
    "        with open(path) as f:\n",
    "            data = json.load(f)\n",
    "            # Flatten to list if not already\n",
    "            records = data if isinstance(data, list) else [data]\n",
    "            keys = set()\n",
    "            for row in records:\n",
    "                keys.update(row.keys())\n",
    "            return keys\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {path}: {e}\")\n",
    "        return set()\n",
    "\n",
    "# Multithreaded collection\n",
    "all_columns = set()\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    for keys in tqdm(executor.map(get_keys_from_file, json_files), total=len(json_files)):\n",
    "        all_columns.update(keys)\n",
    "\n",
    "all_columns = sorted(all_columns)\n",
    "print(f\"Discovered {len(all_columns)} columns.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ab86add",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open(\"merged_output1.csv\", newline='', encoding='utf-8') as f:\n",
    "    reader = csv.reader(f)\n",
    "    all_columns = next(reader)  # this grabs the first row as the header\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26af414c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exporting to CSV:   8%|▊         | 83338/1032524 [11:39<2:12:46, 119.15it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_path \u001b[38;5;129;01min\u001b[39;00m tqdm(json_files, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExporting to CSV\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 17\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     18\u001b[0m             data \u001b[38;5;241m=\u001b[39m orjson\u001b[38;5;241m.\u001b[39mloads(f\u001b[38;5;241m.\u001b[39mread())\n\u001b[0;32m     19\u001b[0m             records \u001b[38;5;241m=\u001b[39m data \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [data]\n",
      "File \u001b[1;32mc:\\Users\\adria\\miniconda3\\envs\\scraper\\lib\\site-packages\\IPython\\core\\interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    308\u001b[0m     )\n\u001b[1;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import orjson\n",
    "from tqdm import tqdm\n",
    "\n",
    "output_csv = \"merged_output.csv\"\n",
    "all_columns_with_file = all_columns + [\"source_file\"]\n",
    "\n",
    "batch_size = 1000\n",
    "rows_batch = []\n",
    "\n",
    "with open(output_csv, 'w', newline='', encoding='utf-8') as out_f:\n",
    "    writer = csv.DictWriter(out_f, fieldnames=all_columns_with_file)\n",
    "    writer.writeheader()\n",
    "\n",
    "    for file_path in tqdm(json_files, desc=\"Exporting to CSV\"):\n",
    "        try:\n",
    "            with open(file_path, 'rb') as f:\n",
    "                data = orjson.loads(f.read())\n",
    "                records = data if isinstance(data, list) else [data]\n",
    "\n",
    "            rows_batch.extend(\n",
    "                {**{col: record.get(col, None) for col in all_columns}, \"source_file\": file_path.name}\n",
    "                for record in records\n",
    "            )\n",
    "\n",
    "            if len(rows_batch) >= batch_size:\n",
    "                writer.writerows(rows_batch)\n",
    "                rows_batch.clear()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error writing {file_path.name}: {e}\")\n",
    "\n",
    "    # write leftover rows\n",
    "    if rows_batch:\n",
    "        writer.writerows(rows_batch)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scraper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
